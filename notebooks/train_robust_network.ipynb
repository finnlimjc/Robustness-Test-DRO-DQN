{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcc8d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5ca0f7",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c71549",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91a93813",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.agent_interface import *\n",
    "from src.agent import *\n",
    "from src.env import *\n",
    "from src.price_data import *\n",
    "from src.prior_measure import *\n",
    "from src.q import *\n",
    "from src.robust import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27028376",
   "metadata": {},
   "source": [
    "# Initialize Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6e2c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '1995-01-01'\n",
    "end_date = '2024-12-31'\n",
    "batch_size = 32\n",
    "\n",
    "stock_params = {\n",
    "    'symbol': 'SPY',\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date\n",
    "}\n",
    "\n",
    "env_params = {\n",
    "    'start_date': start_date,\n",
    "    'end_date': end_date,\n",
    "    'rf_rate': 0.024,\n",
    "    'trans_cost': 0.005,\n",
    "    'batch_size': batch_size,\n",
    "    'logging': True,\n",
    "    'seed': 42\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f5af0f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "yf = YahooFinance(**stock_params)\n",
    "df = yf.pipeline()\n",
    "asset_log_returns = df['log_return'].dropna().to_numpy()\n",
    "env = PortfolioEnv(asset_log_returns=asset_log_returns, **env_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38fbb514",
   "metadata": {},
   "source": [
    "# Initialize Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99bb0b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")\n",
    "\n",
    "#Shared params\n",
    "n_updates = 1\n",
    "state_dim = 63\n",
    "action_dim = 1\n",
    "\n",
    "training_controller_params = {\n",
    "    'train_steps': 1,\n",
    "    'clone_steps': 50,\n",
    "    'batch_size': batch_size,\n",
    "    'n_batches': n_updates,\n",
    "}\n",
    "\n",
    "duality_params = {\n",
    "    'discount_rate': 0.99,\n",
    "    'delta': 1e-4,\n",
    "    'sinkhorn_radius': 0.003,\n",
    "}\n",
    "\n",
    "q_params = {\n",
    "    'input_size': state_dim,\n",
    "    'hidden_size': [64, 64],\n",
    "    'output_size': action_dim\n",
    "}\n",
    "\n",
    "dqn_params = {\n",
    "    'state_dim': state_dim,\n",
    "    'action_dim': action_dim,\n",
    "    'batch_size': batch_size,\n",
    "    'n_updates': 10,\n",
    "    'epsilon': 0.1,\n",
    "    'device': device,\n",
    "    'seed': 123\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de079181",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_controller = TrainingController(**training_controller_params)\n",
    "prior_measure = PriorStudentDistribution(device=device)\n",
    "duality_operator = DualityHQOperator(**duality_params)\n",
    "q = QFunc(**q_params)\n",
    "agent = PORDQN(training_controller=training_controller, prior_measure=prior_measure, duality_operator=duality_operator,\n",
    "               qfunc = q, **dqn_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2668f362",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3df6129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1:   0%|          | 0/7492 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The expanded size of the tensor (128) must match the existing size (32) at non-singleton dimension 0.  Target sizes: [128].  Tensor sizes: [32]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m     agent.agent_end(reward=reward, observation=next_state, info=info)\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     action_idx = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43magent_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m=\u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m step_bar.update(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Finn_Nitro\\Documents\\GitHub\\Robustness-Test-DRO-DQN\\src\\agent_interface.py:28\u001b[39m, in \u001b[36mAgentInterface.agent_step\u001b[39m\u001b[34m(self, reward, observation, info)\u001b[39m\n\u001b[32m     25\u001b[39m action = \u001b[38;5;28mself\u001b[39m.get_action(observation)\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.training_mode:\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_mode_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIS_TERMINAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m# Update previous state and action\u001b[39;00m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mself\u001b[39m.prev_state = observation\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Finn_Nitro\\Documents\\GitHub\\Robustness-Test-DRO-DQN\\src\\agent.py:514\u001b[39m, in \u001b[36mPORDQN.train_mode_actions\u001b[39m\u001b[34m(self, reward, observation, is_terminal, info)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28mself\u001b[39m.training_controller.step_increment()\n\u001b[32m    513\u001b[39m lamda_init = torch.full((\u001b[38;5;28mself\u001b[39m.batch_size, ), \u001b[38;5;28mself\u001b[39m.lamda_init, dtype=torch.float32, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m514\u001b[39m terminal_tensor, risk_free_rate_tensor, transaction_cost_tensor = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_terminal_state_and_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mis_terminal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[38;5;28mself\u001b[39m.buffer.add(\u001b[38;5;28mself\u001b[39m.prev_state, \u001b[38;5;28mself\u001b[39m.prev_action, reward, observation, terminal_tensor, lamda_init, risk_free_rate_tensor, transaction_cost_tensor)\n\u001b[32m    516\u001b[39m sufficient_samples = \u001b[38;5;28mself\u001b[39m.training_controller.has_samples(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.buffer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Finn_Nitro\\Documents\\GitHub\\Robustness-Test-DRO-DQN\\src\\agent.py:492\u001b[39m, in \u001b[36mPORDQN._handle_terminal_state_and_info\u001b[39m\u001b[34m(self, is_terminal, info)\u001b[39m\n\u001b[32m    480\u001b[39m terminal_tensor = torch.full(\n\u001b[32m    481\u001b[39m     (\u001b[38;5;28mself\u001b[39m.batch_size, ),\n\u001b[32m    482\u001b[39m     is_terminal,\n\u001b[32m    483\u001b[39m     dtype=torch.bool,\n\u001b[32m    484\u001b[39m     device=\u001b[38;5;28mself\u001b[39m.device\n\u001b[32m    485\u001b[39m )\n\u001b[32m    487\u001b[39m \u001b[38;5;66;03m# Handle numpy arrays and scalar values through reshape and expand\u001b[39;00m\n\u001b[32m    488\u001b[39m risk_free_rate_tensor = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrisk_free_rate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m492\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m transaction_cost_tensor = torch.as_tensor(\n\u001b[32m    495\u001b[39m     transaction_cost,\n\u001b[32m    496\u001b[39m     dtype=torch.float32,\n\u001b[32m    497\u001b[39m     device=\u001b[38;5;28mself\u001b[39m.device\n\u001b[32m    498\u001b[39m ).reshape(-\u001b[32m1\u001b[39m).expand(\u001b[38;5;28mself\u001b[39m.batch_size)\n\u001b[32m    500\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m terminal_tensor, risk_free_rate_tensor, transaction_cost_tensor\n",
      "\u001b[31mRuntimeError\u001b[39m: The expanded size of the tensor (128) must match the existing size (32) at non-singleton dimension 0.  Target sizes: [128].  Tensor sizes: [32]"
     ]
    }
   ],
   "source": [
    "n_epochs = 1\n",
    "for epoch in range(1 , n_epochs+1):\n",
    "    cum_rewards = np.zeros(shape=(batch_size, 1))\n",
    "    observation, _ = env.reset()\n",
    "    action_idx = agent.agent_start(observation)\n",
    "    steps = env.action_steps\n",
    "    done = np.array([False]*batch_size)\n",
    "    \n",
    "    with tqdm(total=steps, desc=f\"Episode {epoch}\", mininterval=2) as step_bar:\n",
    "        while not done.any():\n",
    "            next_state, reward, done, truncated, info = env.step(action_idx)\n",
    "            cum_rewards += reward\n",
    "            if done.any():\n",
    "                agent.agent_end(reward=reward, observation=next_state, info=info)\n",
    "            else:\n",
    "                action_idx = agent.agent_step(reward=reward, observation=next_state, info=info)\n",
    "            \n",
    "            step_bar.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f636d2c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
