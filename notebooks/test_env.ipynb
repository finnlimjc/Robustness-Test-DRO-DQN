{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f0260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6d8ac",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5389655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.price_data import *\n",
    "from src.env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0408dc",
   "metadata": {},
   "source": [
    "# Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6216f984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>normalized_price</th>\n",
       "      <th>log_price</th>\n",
       "      <th>log_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995-01-03</th>\n",
       "      <td>26.584360</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.280323</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-01-04</th>\n",
       "      <td>26.711391</td>\n",
       "      <td>1.004778</td>\n",
       "      <td>3.285090</td>\n",
       "      <td>0.004767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-01-05</th>\n",
       "      <td>26.711391</td>\n",
       "      <td>1.004778</td>\n",
       "      <td>3.285090</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-01-06</th>\n",
       "      <td>26.738604</td>\n",
       "      <td>1.005802</td>\n",
       "      <td>3.286108</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-01-09</th>\n",
       "      <td>26.765804</td>\n",
       "      <td>1.006825</td>\n",
       "      <td>3.287125</td>\n",
       "      <td>0.001017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                price  normalized_price  log_price  log_return\n",
       "date                                                          \n",
       "1995-01-03  26.584360          1.000000   3.280323         NaN\n",
       "1995-01-04  26.711391          1.004778   3.285090    0.004767\n",
       "1995-01-05  26.711391          1.004778   3.285090    0.000000\n",
       "1995-01-06  26.738604          1.005802   3.286108    0.001018\n",
       "1995-01-09  26.765804          1.006825   3.287125    0.001017"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date='1995-01-01'\n",
    "end_date='2023-12-31'\n",
    "yf = YahooFinance('SPY', start_date=start_date, end_date=end_date)\n",
    "df = yf.pipeline()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366c3ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00476704,  0.        ,  0.00101823, ...,  0.00180638,\n",
       "        0.00037774, -0.0028991 ], shape=(7299,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_returns = df['log_return'].dropna().to_numpy()\n",
    "asset_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b6baa",
   "metadata": {},
   "source": [
    "# Portfolio Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f73f134",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Please use reset() to initialize environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done.all():\n\u001b[32m     10\u001b[39m     actions = torch.randint(\u001b[32m0\u001b[39m, env.action_space.n, (env.batch_size, \u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     next_state, reward, done, _, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     total_rewards += reward.squeeze()\n\u001b[32m     14\u001b[39m     step_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Finn_Nitro\\Documents\\GitHub\\Robustness-Test-DRO-DQN\\src\\env.py:203\u001b[39m, in \u001b[36mPortfolioEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mGenerate the state and update the environment based on the action taken by the agent.\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m \u001b[33;03m    info: the info dictionary that contains interest and transaction cost information.\u001b[39;00m\n\u001b[32m    201\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mposition\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlog_wealth\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mPlease use reset() to initialize environment.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    205\u001b[39m \u001b[38;5;66;03m# Take Action and Calculate Reward\u001b[39;00m\n\u001b[32m    206\u001b[39m action = \u001b[38;5;28mself\u001b[39m._check_action(action)\n",
      "\u001b[31mAttributeError\u001b[39m: Please use reset() to initialize environment."
     ]
    }
   ],
   "source": [
    "# Check if error works\n",
    "# Initialize Environment\n",
    "env = PortfolioEnv(asset_returns, start_date=start_date, end_date=end_date, seed=123)\n",
    "\n",
    "# Run Random Policy\n",
    "done = np.zeros((env.batch_size, 1), dtype=bool)\n",
    "total_rewards = np.zeros(env.batch_size)\n",
    "step_count = 0\n",
    "while not done.all():\n",
    "    actions = torch.randint(0, env.action_space.n, (env.batch_size, 1))\n",
    "    next_state, reward, done, _, info = env.step(actions)\n",
    "    \n",
    "    total_rewards += reward.squeeze()\n",
    "    step_count += 1\n",
    "    print(f\"Step {step_count}: Avg Reward {reward.mean().item():.6f}\")\n",
    "\n",
    "    if step_count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f6876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State Shape: (8, 63)\n",
      "First 5 state values: [-0.00086937  0.00154606 -0.00678914 -0.00085864  0.01174145]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Environment\n",
    "env = PortfolioEnv(asset_returns, start_date=start_date, end_date=end_date, seed=123)\n",
    "state, info = env.reset()\n",
    "print(\"Initial State Shape:\", state.shape)\n",
    "print(\"First 5 state values:\", state[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8493a4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00086937,  0.00154606, -0.00678914, ..., -0.00577501,\n",
       "         0.0150923 , -0.00073009],\n",
       "       [-0.01227176,  0.00704779, -0.00692798, ...,  0.00652163,\n",
       "        -0.000885  ,  0.00230706],\n",
       "       [-0.00919072,  0.00416202,  0.00361918, ..., -0.0048683 ,\n",
       "        -0.00210032,  0.00176851],\n",
       "       ...,\n",
       "       [-0.00373213,  0.0015567 , -0.01604013, ...,  0.00501263,\n",
       "         0.00249704,  0.00262166],\n",
       "       [-0.01801519, -0.00300635,  0.00400721, ..., -0.00811829,\n",
       "         0.00133017, -0.01498039],\n",
       "       [ 0.01151596,  0.00190684,  0.0100173 , ..., -0.00065598,\n",
       "         0.01470323, -0.00338484]], shape=(8, 7299))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Sequence Should be Different Paths\n",
    "env.seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f85dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Avg Reward -0.002781\n",
      "Step 2: Avg Reward -0.001583\n",
      "Step 3: Avg Reward -0.004903\n",
      "Step 4: Avg Reward -0.004177\n",
      "Step 5: Avg Reward -0.009452\n"
     ]
    }
   ],
   "source": [
    "# Run Random Policy\n",
    "done = np.zeros((env.batch_size, 1), dtype=bool)\n",
    "total_rewards = np.zeros(env.batch_size)\n",
    "step_count = 0\n",
    "while not done.all():\n",
    "    actions = torch.randint(0, env.action_space.n, (env.batch_size, 1))\n",
    "    next_state, reward, done, _, info = env.step(actions)\n",
    "    \n",
    "    total_rewards += reward.squeeze()\n",
    "    step_count += 1\n",
    "    print(f\"Step {step_count}: Avg Reward {reward.mean().item():.6f}\")\n",
    "\n",
    "    if step_count == 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
