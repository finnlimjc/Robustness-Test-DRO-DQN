{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f0260f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a6d8ac",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5389655",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.price_data import *\n",
    "from src.env import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0408dc",
   "metadata": {},
   "source": [
    "# Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6216f984",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>normalized_price</th>\n",
       "      <th>log_price</th>\n",
       "      <th>log_return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1995-01-03</th>\n",
       "      <td>26.584364</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.280323</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-01-04</th>\n",
       "      <td>26.711380</td>\n",
       "      <td>1.004778</td>\n",
       "      <td>3.285090</td>\n",
       "      <td>0.004766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-01-05</th>\n",
       "      <td>26.711380</td>\n",
       "      <td>1.004778</td>\n",
       "      <td>3.285090</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-01-06</th>\n",
       "      <td>26.738604</td>\n",
       "      <td>1.005802</td>\n",
       "      <td>3.286108</td>\n",
       "      <td>0.001019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995-01-09</th>\n",
       "      <td>26.765827</td>\n",
       "      <td>1.006826</td>\n",
       "      <td>3.287126</td>\n",
       "      <td>0.001018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                price  normalized_price  log_price  log_return\n",
       "date                                                          \n",
       "1995-01-03  26.584364          1.000000   3.280323         NaN\n",
       "1995-01-04  26.711380          1.004778   3.285090    0.004766\n",
       "1995-01-05  26.711380          1.004778   3.285090    0.000000\n",
       "1995-01-06  26.738604          1.005802   3.286108    0.001019\n",
       "1995-01-09  26.765827          1.006826   3.287126    0.001018"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_date='1995-01-01'\n",
    "end_date='2023-12-31'\n",
    "yf = YahooFinance('SPY', start_date=start_date, end_date=end_date)\n",
    "df = yf.pipeline()\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "366c3ee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00476647,  0.        ,  0.00101866, ...,  0.00180644,\n",
       "        0.00037781, -0.00289917], shape=(7299,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asset_returns = df['log_return'].dropna().to_numpy()\n",
    "asset_returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704b6baa",
   "metadata": {},
   "source": [
    "# Portfolio Enviroment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f73f134",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Please use reset() to initialize environment.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done.all():\n\u001b[32m     10\u001b[39m     actions = torch.randint(\u001b[32m0\u001b[39m, env.action_space.n, (env.batch_size, \u001b[32m1\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m     next_state, reward, done, _, info = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     total_rewards += reward.squeeze()\n\u001b[32m     14\u001b[39m     step_count += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Finn_Nitro\\Documents\\GitHub\\Robustness-Test-DRO-DQN\\src\\env.py:200\u001b[39m, in \u001b[36mPortfolioEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    186\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''\u001b[39;00m\n\u001b[32m    187\u001b[39m \u001b[33;03mGenerate the state and update the environment based on the action taken by the agent.\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[33;03m\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    197\u001b[39m \u001b[33;03m    info: the info dictionary that contains interest and transaction cost information.\u001b[39;00m\n\u001b[32m    198\u001b[39m \u001b[33;03m'''\u001b[39;00m\n\u001b[32m    199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mposition\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mlog_wealth\u001b[39m\u001b[33m'\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mPlease use reset() to initialize environment.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    202\u001b[39m \u001b[38;5;66;03m# Take Action and Calculate Reward\u001b[39;00m\n\u001b[32m    203\u001b[39m action = \u001b[38;5;28mself\u001b[39m._check_action(action)\n",
      "\u001b[31mAttributeError\u001b[39m: Please use reset() to initialize environment."
     ]
    }
   ],
   "source": [
    "# Check if error works\n",
    "# Initialize Environment\n",
    "env = PortfolioEnv(asset_returns, start_date=start_date, end_date=end_date, seed=123)\n",
    "\n",
    "# Run Random Policy\n",
    "done = np.zeros((env.batch_size, 1), dtype=bool)\n",
    "total_rewards = np.zeros(env.batch_size)\n",
    "step_count = 0\n",
    "while not done.all():\n",
    "    actions = torch.randint(0, env.action_space.n, (env.batch_size, 1))\n",
    "    next_state, reward, done, _, info = env.step(actions)\n",
    "    \n",
    "    total_rewards += reward.squeeze()\n",
    "    step_count += 1\n",
    "    print(f\"Step {step_count}: Avg Reward {reward.mean().item():.6f}\")\n",
    "\n",
    "    if step_count == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2f6876d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial State Shape: (8, 63)\n",
      "First 5 state values: [-0.00086901  0.00154616 -0.00678905 -0.00085854  0.01174174]\n"
     ]
    }
   ],
   "source": [
    "# Initialize Environment\n",
    "env = PortfolioEnv(asset_returns, start_date=start_date, end_date=end_date, seed=123)\n",
    "state, info = env.reset()\n",
    "print(\"Initial State Shape:\", state.shape)\n",
    "print(\"First 5 state values:\", state[0, :5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8493a4b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00086901,  0.00154616, -0.00678905, ..., -0.00577493,\n",
       "         0.01509239, -0.00072956],\n",
       "       [-0.01227155,  0.00704765, -0.0069277 , ...,  0.00652123,\n",
       "        -0.00088494,  0.00230706],\n",
       "       [-0.00919051,  0.00416217,  0.00361897, ..., -0.00486801,\n",
       "        -0.00210054,  0.00176878],\n",
       "       ...,\n",
       "       [-0.00373207,  0.00155663, -0.01604029, ...,  0.00501286,\n",
       "         0.00249628,  0.00262207],\n",
       "       [-0.01801578, -0.00300635,  0.00400662, ..., -0.00811829,\n",
       "         0.00133017, -0.01498002],\n",
       "       [ 0.01151607,  0.00190652,  0.01001762, ..., -0.00065648,\n",
       "         0.01470358, -0.00338533]], shape=(8, 7299))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check Sequence Should be Different Paths\n",
    "env.seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98f85dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Avg Reward -0.002661\n",
      "Step 2: Avg Reward -0.000662\n",
      "Step 3: Avg Reward -0.000797\n",
      "Step 4: Avg Reward -0.003142\n",
      "Step 5: Avg Reward 0.000695\n"
     ]
    }
   ],
   "source": [
    "# Run Random Policy\n",
    "done = np.zeros((env.batch_size, 1), dtype=bool)\n",
    "total_rewards = np.zeros(env.batch_size)\n",
    "step_count = 0\n",
    "while not done.all():\n",
    "    actions = torch.randint(0, env.action_space.n, (env.batch_size, 1))\n",
    "    next_state, reward, done, _, info = env.step(actions)\n",
    "    \n",
    "    total_rewards += reward.squeeze()\n",
    "    step_count += 1\n",
    "    print(f\"Step {step_count}: Avg Reward {reward.mean().item():.6f}\")\n",
    "\n",
    "    if step_count == 5:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.11.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
