{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "import pickle\n",
    "import pandas_market_calendars as mcal\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "from gymnasium import spaces\n",
    "from mmd.env import GenLSTM, MMDSimulator, load_generator\n",
    "from mmd.train import start_writer, get_params_from_events, get_params_dicts, get_robustq_params_dicts, train_robustdqn, training_info\n",
    "from mmd.evaluation import simulate_agent_spx\n",
    "from agent.q import QFunc\n",
    "from agent.DQN import PORDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_length = 560\n",
    "burn_in = 500\n",
    "state_len = 60\n",
    "cal_start_date = '1995-01-01'\n",
    "cal_end_date = '2023-12-31'\n",
    "trading_calendar = 'NYSE'\n",
    "calendar = mcal.get_calendar(trading_calendar)\n",
    "schedule = calendar.schedule(start_date=cal_start_date, end_date=cal_end_date)\n",
    "\n",
    "int_rate = 0.024\n",
    "trans_cost = 0.0025 # standard cost = 0.0005\n",
    "eval_batch_size = 1000\n",
    "eval_seed = 12345"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./dataset/lstm/ma_params.pkl', 'rb') as f:\n",
    "    ma_model_params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "events_path = './dataset/lstm/'\n",
    "params = get_params_from_events(events_path)\n",
    "for key, value in params.items():\n",
    "    for key, value in value.items():\n",
    "        if key in globals(): continue # skip if already in globals\n",
    "        globals()[key] = value\n",
    "data_params, model_params, train_params = get_params_dicts(vars().copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "device = 'cpu'\n",
    "action_space = spaces.Discrete(9)\n",
    "action_values = torch.linspace(-1., 1., 9, device=device)\n",
    "num_actions = len(action_values)\n",
    "nu_dist = 't'\n",
    "nu_scale = 0.03\n",
    "nu_df = 2\n",
    "other_state_vars = ['log_wealth', 'positions', 'dt']\n",
    "obs_dim = state_len + len(other_state_vars)\n",
    "\n",
    "discount = 0.99\n",
    "eps_greedy = 0.1 # epsilon greedy parameter\n",
    "buffer_max_length = int(1e5)\n",
    "clone_steps = 50\n",
    "train_steps = 1\n",
    "agent_batch_size = 128\n",
    "n_batches = 1\n",
    "n_epochs = 1\n",
    "robustq_lr = 1e-4\n",
    "architecture = [64, 64]\n",
    "pre_train_Q = False\n",
    "n_episodes = 3\n",
    "\n",
    "norm_ord = 1\n",
    "lamda_init = 0. # initial lambda\n",
    "lamda_max_iter = 100\n",
    "lamda_step_size = 10 # step size for learning rate scheduler\n",
    "lamda_gamma = None # gamma for learning rate scheduler\n",
    "lamda_lr = [0.02 * (10**i) for i in range(lamda_step_size)]\n",
    "n_outer = 1 # not used in this algorithm but used in logging by writer\n",
    "n_inner = 1000 # number of samples from nu to calc inner expectations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta = 1e-4 # regularisation parameter for Sinkhorn distance\n",
    "epsilon = 3e-3 # Sinkhorn distance\n",
    "\n",
    "# also change these if delta and epsilon are changed\n",
    "delta_str = \"1e-4\"\n",
    "eps_str = \"3e-3\"\n",
    "\n",
    "simulator_params, model_params = get_robustq_params_dicts(vars().copy())\n",
    "\n",
    "seeds = [0,1,2,3,4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 6739/6739 [22:39<00:00,  4.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 mean of summed rewards: -1.937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 2: 100%|██████████| 6739/6739 [24:22<00:00,  4.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 mean of summed rewards: -0.554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 3: 100%|██████████| 6739/6739 [22:17<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 mean of summed rewards: -0.261\n"
     ]
    }
   ],
   "source": [
    "seed = seeds[0]\n",
    "name = \"0\"\n",
    "writer = start_writer(simulator_params, model_params, model_name=name)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "generator = GenLSTM(4, 1, 60)\n",
    "generator = load_generator(generator, events_path)\n",
    "robustq = QFunc(state_len+len(other_state_vars), architecture, action_values.shape[0]).to(device)\n",
    "\n",
    "env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in,int_rate, trans_cost, batch_size, action_space, action_values, device)\n",
    "robustdqn_agent = PORDQN(obs_dim, num_actions, discount, nu_scale, nu_df, action_values, epsilon, delta, n_inner, lamda_init,lamda_lr, lamda_max_iter, lamda_step_size, lamda_gamma, norm_ord, robustq, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, robustq_lr, device=device, seed=seed, writer=writer)\n",
    "\n",
    "robustdqn_agent = train_robustdqn(robustdqn_agent, env, writer, simulator_params, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 6739/6739 [22:36<00:00,  4.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 mean of summed rewards: -1.928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 2: 100%|██████████| 6739/6739 [22:42<00:00,  4.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 mean of summed rewards: -0.700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 3: 100%|██████████| 6739/6739 [22:16<00:00,  5.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 mean of summed rewards: -0.187\n"
     ]
    }
   ],
   "source": [
    "seed = seeds[1]\n",
    "name = \"1\"\n",
    "writer = start_writer(simulator_params, model_params, model_name=name)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "generator = GenLSTM(4, 1, 60)\n",
    "generator = load_generator(generator, events_path)\n",
    "robustq = QFunc(state_len+len(other_state_vars), architecture, action_values.shape[0]).to(device)\n",
    "\n",
    "env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in,int_rate, trans_cost, batch_size, action_space, action_values, device)\n",
    "robustdqn_agent = PORDQN(obs_dim, num_actions, discount, nu_scale, nu_df, action_values, epsilon, delta, n_inner, lamda_init,lamda_lr, lamda_max_iter, lamda_step_size, lamda_gamma, norm_ord, robustq, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, robustq_lr, device=device, seed=seed, writer=writer)\n",
    "\n",
    "robustdqn_agent = train_robustdqn(robustdqn_agent, env, writer, simulator_params, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = seeds[2]\n",
    "name = f\"newlr_delta_{delta_str}_eps_{eps_str}_seed_{seed}_txn_0.05\"\n",
    "writer = start_writer(simulator_params, model_params, model_name=name)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "generator = GenLSTM(4, 1, 60)\n",
    "generator = load_generator(generator, events_path)\n",
    "robustq = QFunc(state_len+len(other_state_vars), architecture, action_values.shape[0]).to(device)\n",
    "\n",
    "env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in,int_rate, trans_cost, batch_size, action_space, action_values, device)\n",
    "robustdqn_agent = PORDQN(obs_dim, num_actions, discount, nu_scale, nu_df, action_values, epsilon, delta, n_inner, lamda_init,lamda_lr, lamda_max_iter, lamda_step_size, lamda_gamma, norm_ord, robustq, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, robustq_lr, device=device, seed=seed, writer=writer)\n",
    "\n",
    "robustdqn_agent = train_robustdqn(robustdqn_agent, env, writer, simulator_params, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 3: 100%|██████████| 6739/6739 [24:25<00:00,  4.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 mean of summed rewards: -1.064\n"
     ]
    }
   ],
   "source": [
    "seed = seeds[3]\n",
    "name = f\"newlr_delta_{delta_str}_eps_{eps_str}_seed_{seed}_txn_0.25\"\n",
    "writer = start_writer(simulator_params, model_params, model_name=name)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "generator = GenLSTM(4, 1, 60)\n",
    "generator = load_generator(generator, events_path)\n",
    "robustq = QFunc(state_len+len(other_state_vars), architecture, action_values.shape[0]).to(device)\n",
    "\n",
    "env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in,int_rate, trans_cost, batch_size, action_space, action_values, device)\n",
    "robustdqn_agent = PORDQN(obs_dim, num_actions, discount, nu_scale, nu_df, action_values, epsilon, delta, n_inner, lamda_init,lamda_lr, lamda_max_iter, lamda_step_size, lamda_gamma, norm_ord, robustq, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, robustq_lr, device=device, seed=seed, writer=writer)\n",
    "\n",
    "robustdqn_agent = train_robustdqn(robustdqn_agent, env, writer, simulator_params, model_params, \"runs/newlr_delta_1e-4_eps_3e-3_seed_3_txn_0.05/checkpoint.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 1: 100%|██████████| 6739/6739 [22:21<00:00,  5.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 mean of summed rewards: -1.458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 2: 100%|██████████| 6739/6739 [24:57<00:00,  4.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 mean of summed rewards: -0.724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 3: 100%|██████████| 6739/6739 [22:24<00:00,  5.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 mean of summed rewards: -0.224\n"
     ]
    }
   ],
   "source": [
    "seed = seeds[4]\n",
    "name = \"4\"\n",
    "writer = start_writer(simulator_params, model_params, model_name=name)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "generator = GenLSTM(4, 1, 60)\n",
    "generator = load_generator(generator, events_path)\n",
    "robustq = QFunc(state_len+len(other_state_vars), architecture, action_values.shape[0]).to(device)\n",
    "\n",
    "env = MMDSimulator(generator, ma_model_params, trading_calendar, cal_start_date, cal_end_date, state_len, burn_in,int_rate, trans_cost, batch_size, action_space, action_values, device)\n",
    "robustdqn_agent = PORDQN(obs_dim, num_actions, discount, nu_scale, nu_df, action_values, epsilon, delta, n_inner, lamda_init,lamda_lr, lamda_max_iter, lamda_step_size, lamda_gamma, norm_ord, robustq, eps_greedy, buffer_max_length, clone_steps, train_steps, agent_batch_size, n_batches, n_epochs, robustq_lr, device=device, seed=seed, writer=writer)\n",
    "\n",
    "robustdqn_agent = train_robustdqn(robustdqn_agent, env, writer, simulator_params, model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
